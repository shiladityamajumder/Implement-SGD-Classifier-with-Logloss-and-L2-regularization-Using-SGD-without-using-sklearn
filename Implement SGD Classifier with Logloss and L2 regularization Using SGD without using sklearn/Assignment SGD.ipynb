{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7eiDWcM_MC3H"
   },
   "source": [
    "# <font color='red'>Implement SGD Classifier with Logloss and L2 regularization Using SGD without using sklearn</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yfe2NTQtLq11"
   },
   "source": [
    "**There will be some functions that start with the word \"grader\" ex: grader_weights(), grader_sigmoid(), grader_logloss() etc, you should not change those function definition.<br><br>Every Grader function has to return True.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fk5DSPCLxqT-"
   },
   "source": [
    "<font color='red'> Importing packages</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "42Et8BKIxnsp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NpSk3WQBx7TQ"
   },
   "source": [
    "<font color='red'>Creating custom dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BsMp0oWzx6dv"
   },
   "outputs": [],
   "source": [
    "# please don't change random_state\n",
    "X, y = make_classification(n_samples=50000, n_features=15, n_informative=10, n_redundant=5,\n",
    "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)\n",
    "# make_classification is used to create custom dataset \n",
    "# Please check this link (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L8W2fg1cyGdX",
    "outputId": "029d4c84-03b2-4143-a04c-34ff49c88890"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 15), (50000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x99RWCgpqNHw"
   },
   "source": [
    "<font color='red'>Splitting data into train and test </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Kh4dBfVyJMP"
   },
   "outputs": [],
   "source": [
    "#please don't change random state\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0DR_YMBsyOci",
    "outputId": "732014d9-1731-4d3f-918f-a9f5255ee149"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37500, 15), (37500,), (12500, 15), (12500,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BW4OHswfqjHR"
   },
   "source": [
    "# <font color='red' size=5>SGD classifier</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "3HpvTwDHyQQy",
    "outputId": "5729f08c-079a-4b17-bf51-f9aeb5abb13b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0001, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='constant', loss='log', max_iter=None, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
       "       tol=0.001, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alpha : float\n",
    "# Constant that multiplies the regularization term. \n",
    "\n",
    "# eta0 : double\n",
    "# The initial learning rate for the ‘constant’, ‘invscaling’ or ‘adaptive’ schedules.\n",
    "\n",
    "clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
    "clf\n",
    "# Please check this documentation (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "colab_type": "code",
    "id": "YYaVyQ2lyXcr",
    "outputId": "dc0bf840-b37e-4552-e513-84b64f6c64c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.76, NNZs: 15, Bias: -0.314605, T: 37500, Avg. loss: 0.455801\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.92, NNZs: 15, Bias: -0.469578, T: 75000, Avg. loss: 0.394737\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.98, NNZs: 15, Bias: -0.580452, T: 112500, Avg. loss: 0.385561\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.02, NNZs: 15, Bias: -0.660824, T: 150000, Avg. loss: 0.382161\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.04, NNZs: 15, Bias: -0.717218, T: 187500, Avg. loss: 0.380474\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.06, NNZs: 15, Bias: -0.761816, T: 225000, Avg. loss: 0.379481\n",
      "Total training time: 0.03 seconds.\n",
      "Convergence after 6 epochs took 0.03 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0001, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='constant', loss='log', max_iter=None, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
       "       tol=0.001, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X=X_train, y=y_train) # fitting our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "EAfkVI6GyaRO",
    "outputId": "bc88f920-6531-4106-9b4c-4dabb6d72b47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.41177431,  0.18416782, -0.13895073,  0.33572511, -0.18423237,\n",
       "          0.5494352 , -0.45213692, -0.08857465,  0.21536661,  0.17351757,\n",
       "          0.18480827,  0.00443463, -0.07033001,  0.33683181,  0.02004129]]),\n",
       " (1, 15),\n",
       " array([-0.76181561]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_, clf.coef_.shape, clf.intercept_\n",
    "#clf.coef_ will return the weights\n",
    "#clf.coef_.shape will return the shape of weights\n",
    "#clf.intercept_ will return the intercept term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_-CcGTKgsMrY"
   },
   "source": [
    "## <font color='red' size=5> Implement Logistic Regression with L2 regularization Using SGD: without using sklearn </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W1_8bdzitDlM"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "1.  We will be giving you some functions, please write code in that functions only.\n",
    "\n",
    "2.  After every function, we will be giving you expected output, please make sure that you get that output. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zU2Y3-FQuJ3z"
   },
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "* Initialize the weight_vector and intercept term to zeros (Write your code in <font color='blue'>def initialize_weights()</font>)\n",
    "\n",
    "* Create a loss function (Write your code in <font color='blue'>def logloss()</font>) \n",
    "\n",
    " $log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$\n",
    "- for each epoch:\n",
    "\n",
    "    - for each batch of data points in train: (keep batch size=1)\n",
    "\n",
    "        - calculate the gradient of loss function w.r.t each weight in weight vector (write your code in <font color='blue'>def gradient_dw()</font>)\n",
    "\n",
    "        $dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)})$ <br>\n",
    "\n",
    "        - Calculate the gradient of the intercept (write your code in <font color='blue'> def gradient_db()</font>) <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>check this</a>\n",
    "\n",
    "           $ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t}))$\n",
    "\n",
    "        - Update weights and intercept (check the equation number 32 in the above mentioned <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>pdf</a>): <br>\n",
    "        $w^{(t+1)}← w^{(t)}+α(dw^{(t)}) $<br>\n",
    "\n",
    "        $b^{(t+1)}←b^{(t)}+α(db^{(t)}) $\n",
    "    - calculate the log loss for train and test with the updated weights (you can check the python assignment 10th question)\n",
    "    - And if you wish, you can compare the previous loss and the current loss, if it is not updating, then\n",
    "        you can stop the training\n",
    "    - append this loss in the list ( this will be used to see how loss is changing for each epoch after the training is over )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZR_HgjgS_wKu"
   },
   "source": [
    "<font color='blue'>Initialize weights </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GecwYV9fsKZ9"
   },
   "outputs": [],
   "source": [
    "def initialize_weights(dim):\n",
    "    ''' In this function, we will initialize our weights and bias'''\n",
    "    #initialize the weights to zeros array of (dim,1) dimensions\n",
    "    #you use zeros_like function to initialize zero, check this link https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros_like.html\n",
    "    #initialize bias to zero\n",
    "    w=np.zeros_like(dim)\n",
    "    b=0\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37500, 15)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A7I6uWBRsKc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.57349184 -0.19015688 -0.06584143 -0.86990562 -2.80927706 -1.43345052\n",
      "  0.35862361  0.24627836 -2.25803168 -0.87761289  2.31023199 -0.3484947\n",
      " -2.2575668  -1.93628665  1.65242231]\n",
      "w = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "b = 0\n"
     ]
    }
   ],
   "source": [
    "dim=X_train[0] \n",
    "print(dim)\n",
    "w,b = initialize_weights(dim)\n",
    "print('w =',(w))\n",
    "print('b =',str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4MI5SAjP9ofN"
   },
   "source": [
    "<font color='cyan'>Grader function - 1 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pv1llH429wG5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim=X_train[0] \n",
    "w,b = initialize_weights(dim)\n",
    "def grader_weights(w,b):\n",
    "    assert((len(w)==len(dim)) and b==0 and np.sum(w)==0.0)\n",
    "    return True\n",
    "grader_weights(w,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QN83oMWy_5rv"
   },
   "source": [
    "<font color='blue'>Compute sigmoid </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qPv4NJuxABgs"
   },
   "source": [
    "$sigmoid(z)= 1/(1+exp(-z))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nAfmQF47_Sd6"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def sigmoid(z):\n",
    "    ''' In this function, we will return sigmoid of z'''\n",
    "    # compute sigmoid(z) and return\n",
    "    z=1/(1 + math.exp(-z))\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9YrGDwg3Ae4m"
   },
   "source": [
    "<font color='cyan'>Grader function - 2</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P_JASp_NAfK_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_sigmoid(z):\n",
    "    val=sigmoid(z)\n",
    "    assert(val==0.8807970779778823)\n",
    "    return True\n",
    "grader_sigmoid(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gS7JXbcrBOFF"
   },
   "source": [
    "<font color='blue'> Compute loss </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lfEiS22zBVYy"
   },
   "source": [
    "$log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VaFDgsp3sKi6"
   },
   "outputs": [],
   "source": [
    "def logloss(y_true,y_pred):\n",
    "    '''In this function, we will compute log loss '''\n",
    "    sum_opt = 0.0\n",
    "    length = len(y_true)\n",
    "    for i in range(length):\n",
    "        sum_opt = sum_opt + ((y_true[i]*math.log(y_pred[i],10)) + ((1-y_true[i])*math.log((1-y_pred[i]),10)))\n",
    "    loss_calc = (-1/length)*sum_opt\n",
    "    return loss_calc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zs1BTXVSClBt"
   },
   "source": [
    "<font color='cyan'>Grader function - 3 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LzttjvBFCuQ5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_logloss(true,pred):\n",
    "    loss=logloss(true,pred)\n",
    "    assert(loss==0.07644900402910387)\n",
    "    return True\n",
    "true=[1,1,0,1,0]\n",
    "pred=[0.9,0.8,0.1,0.8,0.2]\n",
    "grader_logloss(true,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tQabIadLCBAB"
   },
   "source": [
    "<font color='blue'>Compute gradient w.r.to  'w' </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YTMxiYKaCQgd"
   },
   "source": [
    "$dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)}$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NMVikyuFsKo5"
   },
   "outputs": [],
   "source": [
    "def gradient_dw(x,y,w,b,alpha,N):\n",
    "    '''In this function, we will compute the gardient w.r.to w '''\n",
    "    #dw = x*(y - sigmoid(np.add(np.dot(np.transpose(w),x),b))) - (alpha/N)*w\n",
    "    dw = x*(y-sigmoid(np.dot(w.T, x) + b)) - (alpha/N)*w\n",
    "    return dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RUFLNqL_GER9"
   },
   "source": [
    "<font color='cyan'>Grader function - 4 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WI3xD8ctGEnJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.03932417 -1.65802126  0.39552179  1.93522773  0.57391643  1.40717219\n",
      "  0.43385535  0.02036643 -0.42413939 -0.99725862 -1.83576236 -0.00725938\n",
      " -1.00531444 -0.03686952  2.77293046]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_dw(x,y,w,b,alpha,N):\n",
    "    grad_dw=gradient_dw(x,y,w,b,alpha,N)\n",
    "    print(grad_dw)\n",
    "    assert(np.sum(grad_dw)==2.613689585)\n",
    "    return True\n",
    "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
    "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
    "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
    "grad_y=0\n",
    "grad_w,grad_b=initialize_weights(grad_x)\n",
    "alpha=0.0001\n",
    "N=len(X_train)\n",
    "grader_dw(grad_x,grad_y,grad_w,grad_b,alpha,N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LE8g84_GI62n"
   },
   "source": [
    "<font color='blue'>Compute gradient w.r.to 'b' </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fHvTYZzZJJ_N"
   },
   "source": [
    "$ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0nUf2ft4EZp8"
   },
   "outputs": [],
   "source": [
    "def gradient_db(x,y,w,b):\n",
    "    '''In this function, we will compute gradient w.r.to b '''\n",
    "    #db = (y - sigmoid(np.dot(w.T,x),b))\n",
    "    db = (y-sigmoid(np.dot(w.T, x) + b))\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pbcBzufVG6qk"
   },
   "source": [
    "<font color='cyan'>Grader function - 5 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TfFDKmscG5qZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_db(x,y,w,b):\n",
    "    grad_db=gradient_db(x,y,w,b)\n",
    "    print(grad_db)\n",
    "    assert(grad_db==-0.5)\n",
    "    return True\n",
    "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
    "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
    "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
    "grad_y=0\n",
    "grad_w,grad_b=initialize_weights(grad_x)\n",
    "alpha=0.0001\n",
    "N=len(X_train)\n",
    "grader_db(grad_x,grad_y,grad_w,grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TCK0jY_EOvyU"
   },
   "source": [
    "<font color='blue'> Implementing logistic regression</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6978933333333335\n",
      "1.6986400000000001\n"
     ]
    }
   ],
   "source": [
    "def pred(w,b,X):\n",
    "    N = len(X)\n",
    "    predict = []\n",
    "    for i in range(N):\n",
    "        z=np.dot(w,X[i])+b\n",
    "        if sigmoid(z) >= 0.5: # sigmoid(w,x,b) returns 1/(1+exp(-(dot(x,w)+b)))\n",
    "            predict.append(1)\n",
    "        else:\n",
    "            predict.append(0)\n",
    "    return np.array(predict)\n",
    "print(1-np.sum(y_train - pred(w,b,X_train))/len(X_train))\n",
    "print(1-np.sum(y_test  - pred(w,b,X_test))/len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dmAdc5ejEZ25"
   },
   "outputs": [],
   "source": [
    "def train(X_train,y_train,X_test,y_test,epochs,alpha,eta0):\n",
    "    ''' In this function, we will implement logistic regression'''\n",
    "    #Here eta0 is learning rate\n",
    "    #implement the code as follows\n",
    "    # initalize the weights (call the initialize_weights(X_train[0]) function)\n",
    "    # for every epoch\n",
    "        # for every data point(X_train,y_train)\n",
    "           #compute gradient w.r.to w (call the gradient_dw() function)\n",
    "           #compute gradient w.r.to b (call the gradient_db() function)\n",
    "           #update w, b\n",
    "        # predict the output of x_train[for all data points in X_train] using w,b\n",
    "        #compute the loss between predicted and actual values (call the loss function)\n",
    "        # store all the train loss values in a list\n",
    "        # predict the output of x_test[for all data points in X_test] using w,b\n",
    "        #compute the loss between predicted and actual values (call the loss function)\n",
    "        # store all the test loss values in a list\n",
    "        # you can also compare previous loss and current loss, if loss is not updating then stop the process and return w,b\n",
    "\n",
    "    w, b = initialize_weights(X_train[0])\n",
    "    train_logloss = []\n",
    "    test_logloss = []\n",
    "    for j in range(epochs):\n",
    "        for i in range(X_train.shape[0]):\n",
    "            dw = gradient_dw(X_train[i], y_train[i], w, b, alpha, N)\n",
    "            db = gradient_db(X_train[i], y_train[i], w, b)\n",
    "            w = w + eta0*dw\n",
    "            b = b + eta0*db\n",
    "        y_pred = [sigmoid(np.dot(w,x)+b) for x in X_train]\n",
    "        train_logloss.append(logloss(y_train,y_pred))\n",
    "        \n",
    "        y_pred_test = [sigmoid(np.dot(w,x)+b) for x in X_test]\n",
    "        test_logloss.append(logloss(y_test,y_pred_test))\n",
    "    return w,b,test_logloss, train_logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sUquz7LFEZ6E"
   },
   "outputs": [],
   "source": [
    "alpha=0.0001\n",
    "eta0=0.0001\n",
    "N=len(X_train)\n",
    "epochs=50\n",
    "w,b,train_logloss, test_logloss=train(X_train,y_train,X_test,y_test,epochs,alpha,eta0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l4Zf_wPARlwY"
   },
   "source": [
    "<font color='red'>Goal of assignment</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l3eF_VSPSH2z"
   },
   "source": [
    "Compare your implementation and SGDClassifier's the weights and intercept, make sure they are as close as possible i.e difference should be in terms of 10^-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nx8Rs9rfEZ1R"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.41177431,  0.18416782, -0.13895073,  0.33572511, -0.18423237,\n",
       "          0.5494352 , -0.45213692, -0.08857465,  0.21536661,  0.17351757,\n",
       "          0.18480827,  0.00443463, -0.07033001,  0.33683181,  0.02004129]]),\n",
       " (1, 15),\n",
       " array([-0.76181561]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_, clf.coef_.shape, clf.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.01801812,  0.00886739, -0.00951921,  0.00236852, -0.03704996,\n",
       "          0.02051372,  0.00695529, -0.00132938,  0.00646289,  0.00031207,\n",
       "          0.01394019, -0.00501893, -0.0110041 ,  0.00225829,  0.00294667]]),\n",
       " array([-0.13043656]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are the results we got after we implemented sgd and found the optimal weights and intercept\n",
    "w-clf.coef_, b-clf.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "230YbSgNSUrQ"
   },
   "source": [
    "<font color='blue'>Plot epoch number vs train , test loss </font>\n",
    "\n",
    "* epoch number on X-axis\n",
    "* loss on Y-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1O6GrRt7UeCJ"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(list(range(50)),  train_logloss, label='train_loss = '+str(1-np.sum(y_train - pred(w,b,X_train))/len(X_train)))\n",
    "plt.plot(list(range(50)),  test_logloss, label='test_loss = '+str(1-np.sum(y_test - pred(w,b,X_test))/len(X_test)))\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('log loss')\n",
    "plt.title('elbow plot')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FUN8puFoEZtU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9522133333333334\n",
      "0.95\n"
     ]
    }
   ],
   "source": [
    "def pred(w,b, X):\n",
    "    N = len(X)\n",
    "    predict = []\n",
    "    for i in range(N):\n",
    "        z=np.dot(w,X[i])+b\n",
    "        if sigmoid(z) >= 0.5: # sigmoid(w,x,b) returns 1/(1+exp(-(dot(x,w)+b)))\n",
    "            predict.append(1)\n",
    "        else:\n",
    "            predict.append(0)\n",
    "    return np.array(predict)\n",
    "print(1-np.sum(y_train - pred(w,b,X_train))/len(X_train))\n",
    "print(1-np.sum(y_test  - pred(w,b,X_test))/len(X_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
